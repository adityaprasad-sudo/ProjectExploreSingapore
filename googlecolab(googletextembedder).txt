# --- INSTALL STUFF ---
print("instaling libs")
!pip install -q langchain-community pypdf tqdm langchain-google-genai faiss-cpu requests==2.32.4

# VERY IMPORTANT use this python file if yoy are using google's text embedder

import os
import zipfile
import time
import getpass
from tqdm import tqdm
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS

# getting the api key securely
if "GOOGLE_API_KEY" not in os.environ:
    os.environ["GOOGLE_API_KEY"] = getpass.getpass("google api key: ")

# check if zip exists and unzip it
if os.path.exists("singapore_data.zip"):
    print("unzipping zip")
    with zipfile.ZipFile("singapore_data.zip", 'r') as zip_ref:
        zip_ref.extractall(".")#extracting
    print("unzip done")
else:
    print("cant find 'singapore_data.zip', asuming files are already here")

# setting up model
print("setting up google model")
# using geminiembedding001
embeddings = GoogleGenerativeAIEmbeddings(model="gemini-embedding-001")

print("scanning folders for pdfs")
pdf_files = []
# find files
for root, dirs, files in os.walk("."):
    for file in files:
        if file.endswith(".pdf"):
            # ignore config folders
            if ".config" not in root:
                pdf_files.append(os.path.join(root, file))

if len(pdf_files) == 0:
    print("error: no pdfs found anywhere")
    exit()

print(f"found {len(pdf_files)} pdf documents")
documents = []
for pdf_file in pdf_files:
    try:
        loader = PyPDFLoader(pdf_file)
        documents.extend(loader.load())
    except:
        # skip if its corrupted
        continue

if not documents:
    print("error: found pdfs bu couldnt read em")
    exit()

print(f"total pages: {len(documents)}")

print("text to small chunks")
# i am gonna keep chunk size to be 1000 as that is enough
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
chunks = text_splitter.split_documents(documents)
print(f"total chunks: {len(chunks)}")

# build vector database
print("building vector daabase with google api")

# google api has api limits so i cant send everything at once
# doing it ini samll batches so that it doesnt crash
batch_size = 100
vectorstore = None

for i in tqdm(range(0, len(chunks), batch_size), desc="embedding batchs"):
    batch = chunks[i : i + batch_size]
    try:
        if vectorstore is None:
            # initialize the embeddings
            vectorstore = FAISS.from_documents(batch, embeddings)
        else:
            # If already initialized, just add
            vectorstore.add_documents(batch)
        time.sleep(1) # Standard throttle
    except Exception as e:
        print(f"\nError on batch starting at index {i}: {e}")
        print("wait 30 seconds the code is retrying")
        time.sleep(30) # Increase sleep on failure
        
        # Try to retry the failed batch one more time
        try:
            if vectorstore is None:
                vectorstore = FAISS.from_documents(batch, embeddings)
            else:
                vectorstore.add_documents(batch)
        except:
            print(f"Skipping batch {i} permanently.")
            continue

# save and zip the result
if vectorstore is not None:
    print("saving index locally")
    # naming it google so i know which one it is
    vectorstore.save_local("faiss_index_google")

    print("making a zip")                                                    # fixed(was not able to save the zip file)
    !zip -r faiss_index_google.zip faiss_index_google

    print(" downloaded 'faiss_index_google.zip'")
